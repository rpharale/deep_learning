{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b05f0e-b471-4abe-8d7d-4ea062e5db03",
   "metadata": {},
   "source": [
    "# Finetuning BERT for movie review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bde89-4cca-41b3-bd60-096b166ae735",
   "metadata": {},
   "source": [
    "Docs: https://huggingface.co/docs/transformers/model_doc/distilbert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ca9bf-ae61-4fee-a8ae-b5276464c861",
   "metadata": {},
   "source": [
    "> The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adce1162-715e-4724-acd1-9355bda68bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: watermark in /home/ubuntu/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: ipython in /usr/lib/python3/dist-packages (from watermark) (7.13.0)\n",
      "Requirement already satisfied: pexpect in /usr/lib/python3/dist-packages (from ipython->watermark) (4.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (1.12.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install watermark\n",
    "%pip install torch\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d339a82a-963f-45bb-b021-ed47d701c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: rpharale\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 7.13.0\n",
      "\n",
      "torch       : 1.12.1\n",
      "transformers: 4.25.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'rpharale' -v -p torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716894df-91c3-4c86-ba25-47f3a2ea7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12098e-ce63-497f-b4ba-ae4fa3b0ae11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Env Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c243c91-7048-423d-b02c-27696075fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 142\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220ecdb-0278-4f78-b216-7973e3c12e9f",
   "metadata": {},
   "source": [
    "## Fetch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a36f77-6f19-4dbf-811d-b3e5604d4368",
   "metadata": {},
   "source": [
    "Download the IMDB movie review dataset from https://ai.stanford.edu/~amaas/data/sentiment/ \\\n",
    "Download the csv version from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec51f7f-46e8-4dce-b7d9-035d4a3f913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/rpharale/data/main/dataset/NLP/IMDB_Movie_Review/IMDB_Dataset.csv.zip\"\n",
    "csv_filepath = 'IMDB_Dataset.csv'\n",
    "\n",
    "filename = os.path.basename(url)\n",
    "\n",
    "# Download the zip file\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as f_zip:\n",
    "    with open('IMDB_Dataset.csv', 'wb')  as f_csv:\n",
    "        f_csv.write(f_zip.read(csv_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0a9f11-4e7d-4944-a595-5c824a0efedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6658399b-996b-4df8-8406-f88121730570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd77569d-5f32-406f-8653-dfbc7bb21fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sentiment'] == 'positive', 'sentiment'] = 1\n",
    "df.loc[df['sentiment'] == 'negative', 'sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa1a1d1-4e66-43b2-b19b-5473179a7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...         1\n",
       "1  A wonderful little production. <br /><br />The...         1\n",
       "2  I thought this was a wonderful way to spend ti...         1\n",
       "3  Basically there's a family where a little boy ...         0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b042708-3cd5-474d-a69f-769b5cd5312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fab75c-6dad-4393-9cd6-91480de33a88",
   "metadata": {},
   "source": [
    "## Split the dataset into train, test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ed5a26-e421-4e23-ad6a-75b3b74801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "val_texts = df.iloc[35000:40000]['review'].values\n",
    "val_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:50000]['review'].values\n",
    "test_labels = df.iloc[40000:50000]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313e7f6-f842-4bc7-964a-8a94e1d73f2f",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba89f953-c097-4a21-a77f-24acb79101c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8f56b7bd934e9283e175e53db7b1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7030ece20b5c402f899ffc78e7f4ff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a5eac4f94c4be886310ae4e4a9fc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343944e1b0064e1eb89d4fd4a9fdccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f387bec9-1760-47eb-8916-8eacd74be1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c5291f-d364-41a3-8a03-9d76f30bc4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85a9e9-1705-45f5-9596-3fd21664fdb6",
   "metadata": {},
   "source": [
    "## Dataset Class and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c48d410-171f-427d-8545-ba99ec9d145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDBDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDBDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c612ce-b32a-466e-be56-e5b448bc1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513bfd2-9c3f-4b1a-bfda-97194c634b22",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36357dad-a611-4a79-9604-5a21df27a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5f85fc86f4a9785fab5d6c04fafa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ce413-8f5f-4b74-b3fc-1d2dfda0dc04",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc2c53e-4b82-48ca-b5bf-b0df26b05ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            # batch data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "            \n",
    "            _, pred_labels = torch.max(logits, 1)\n",
    "            correct_pred += (pred_labels == labels).sum()\n",
    "            num_examples += len(labels)\n",
    "    \n",
    "    return correct_pred.float() / num_examples * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5cd83c-0101-4695-801b-d890133b8528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Epoch: 0001/0003,                   Batch: 0001/2188,                   Loss: 0.6950                   \n",
      "                   Epoch: 0001/0003,                   Batch: 0201/2188,                   Loss: 0.4441                   \n",
      "                   Epoch: 0001/0003,                   Batch: 0401/2188,                   Loss: 0.2589                   \n",
      "                   Epoch: 0001/0003,                   Batch: 0601/2188,                   Loss: 0.0580                   \n",
      "                   Epoch: 0001/0003,                   Batch: 0801/2188,                   Loss: 0.5265                   \n",
      "                   Epoch: 0001/0003,                   Batch: 1001/2188,                   Loss: 0.3484                   \n",
      "                   Epoch: 0001/0003,                   Batch: 1201/2188,                   Loss: 1.0605                   \n",
      "                   Epoch: 0001/0003,                   Batch: 1401/2188,                   Loss: 0.2074                   \n",
      "                   Epoch: 0001/0003,                   Batch: 1601/2188,                   Loss: 0.2908                   \n",
      "                   Epoch: 0001/0003,                   Batch: 1801/2188,                   Loss: 0.1516                   \n",
      "                   Epoch: 0001/0003,                   Batch: 2001/2188,                   Loss: 0.0948                   \n",
      "Training accuracy: 95.72%,              Val accuracy: 91.90%              \n",
      "Time elapsed: 13.21 min\n",
      "                   Epoch: 0002/0003,                   Batch: 0001/2188,                   Loss: 0.0953                   \n",
      "                   Epoch: 0002/0003,                   Batch: 0201/2188,                   Loss: 0.0572                   \n",
      "                   Epoch: 0002/0003,                   Batch: 0401/2188,                   Loss: 0.0250                   \n",
      "                   Epoch: 0002/0003,                   Batch: 0601/2188,                   Loss: 0.0465                   \n",
      "                   Epoch: 0002/0003,                   Batch: 0801/2188,                   Loss: 0.3048                   \n",
      "                   Epoch: 0002/0003,                   Batch: 1001/2188,                   Loss: 0.0079                   \n",
      "                   Epoch: 0002/0003,                   Batch: 1201/2188,                   Loss: 0.0230                   \n",
      "                   Epoch: 0002/0003,                   Batch: 1401/2188,                   Loss: 0.0501                   \n",
      "                   Epoch: 0002/0003,                   Batch: 1601/2188,                   Loss: 0.6390                   \n",
      "                   Epoch: 0002/0003,                   Batch: 1801/2188,                   Loss: 0.1452                   \n",
      "                   Epoch: 0002/0003,                   Batch: 2001/2188,                   Loss: 0.0355                   \n",
      "Training accuracy: 97.30%,              Val accuracy: 90.90%              \n",
      "Time elapsed: 26.43 min\n",
      "                   Epoch: 0003/0003,                   Batch: 0001/2188,                   Loss: 0.0457                   \n",
      "                   Epoch: 0003/0003,                   Batch: 0201/2188,                   Loss: 0.0053                   \n",
      "                   Epoch: 0003/0003,                   Batch: 0401/2188,                   Loss: 0.0126                   \n",
      "                   Epoch: 0003/0003,                   Batch: 0601/2188,                   Loss: 0.0021                   \n",
      "                   Epoch: 0003/0003,                   Batch: 0801/2188,                   Loss: 0.0236                   \n",
      "                   Epoch: 0003/0003,                   Batch: 1001/2188,                   Loss: 0.0946                   \n",
      "                   Epoch: 0003/0003,                   Batch: 1201/2188,                   Loss: 0.0928                   \n",
      "                   Epoch: 0003/0003,                   Batch: 1401/2188,                   Loss: 0.3692                   \n",
      "                   Epoch: 0003/0003,                   Batch: 1601/2188,                   Loss: 0.0075                   \n",
      "                   Epoch: 0003/0003,                   Batch: 1801/2188,                   Loss: 0.0052                   \n",
      "                   Epoch: 0003/0003,                   Batch: 2001/2188,                   Loss: 0.0013                   \n",
      "Training accuracy: 97.65%,              Val accuracy: 90.70%              \n",
      "Time elapsed: 39.64 min\n",
      "Total training time: 39.64 min\n",
      "Test accuracy: 90.70%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        # data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = output['loss'], output['logits']\n",
    "        \n",
    "        # Backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # Logging\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\" \\\n",
    "                  Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d}, \\\n",
    "                  Batch: {batch_idx+1:04d}/{len(train_loader):04d}, \\\n",
    "                  Loss: {loss:.4f} \\\n",
    "                  \")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(f\"Training accuracy: {eval_model(model, train_loader, device):.2f}%,\\\n",
    "              Val accuracy: {eval_model(model, val_loader, device):.2f}% \\\n",
    "             \")\n",
    "    \n",
    "    print(f\"Time elapsed: {(time.time() - start_time)/60.0:.2f} min\")\n",
    "\n",
    "print(f\"Total training time: {(time.time() - start_time)/60.0:.2f} min\")\n",
    "print(f\"Test accuracy: {eval_model(model, test_loader, device):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eceb767-3848-4f51-bd3a-b6055bbff993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "model.save_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c028485-0e36-4e89-9ef5-ed687b872210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
